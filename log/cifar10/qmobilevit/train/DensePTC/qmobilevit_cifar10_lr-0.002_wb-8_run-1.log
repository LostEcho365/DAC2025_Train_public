/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
[38;21m2024-11-12 20:50:27,970 - train.py[line:234] - INFO: dataset:
  name: cifar10
  root: /home/xinyuzh/workspace/dataset/DAC2025_Train_public/cifar10
  train_valid_split_ratio: [0.9, 0.1]
  train_valid_split_seed: 1
  resize_mode: bicubic
  center_crop: 32
  n_test_samples: None
  n_valid_samples: None
  num_workers: 2
  img_height: 32
  img_width: 32
  in_channels: 3
  num_classes: 10
  transform: augmented
  shuffle: 0
  augment: None
criterion:
  name: ce
aux_criterion: None
optimizer:
  name: adamw
  lr: 0.002
  weight_decay: 0.0001
  grad_clip_value: 0
scheduler:
  name: cosine
  lr_gamma: 0.99
  lr_min: 2e-05
run:
  experiment: cifar10_qmobilevitmoe_train
  n_epochs: 100
  batch_size: 8
  use_cuda: 1
  gpu_id: 0
  deterministic: 22
  random_state: 42
  log_interval: 200
  train_noise: 0
  grad_clip: False
  max_grad_value: 1
  do_distill: False
  compile: False
quantize:
  weight_bit: 8
  input_bit: 6
noise:
  phase_bias: 0
  phase_noise_std: 0
  gamma_noise_std: 0
  crosstalk_factor: 0
  random_state: 42
  weight_noise_std: 0.0
  output_noise_std: 0
  crosstalk_flag: False
  noise_flag: False
  light_redist: False
  input_power_gating: False
  input_modulation_ER: 6
  output_power_gating: False
  crosstalk_scheduler:
    interv_h: 20
    interv_v: 120
    interv_s: 9
    ps_width: 6
checkpoint:
  save_best_model_k: 3
  checkpoint_dir: cifar10/qmobileViTMoE/train
  model_comment: lr-0.0020_wb-8_run-1
  resume: 0
  restore_checkpoint: 
  no_linear: 0
model:
  name: QMobileViT
  conv_cfg:
    type: QConv2d
    w_bit: 8
    in_bit: 8
    out_bit: 8
  linear_cfg:
    type: QLinear
    w_bit: 8
    in_bit: 8
    out_bit: 8
  norm_cfg:
    type: BN2d
    affine: True
  act_cfg:
    type: ReLU6
    inplace: True
  dim: [144, 192, 240]
  depth: [2, 4, 3]
  channels: [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]
  is_moe: True
  expert_num: 8
  top_k: 2
  expansion: 4
  matmul_cfg:
    type: QMatMul
    w_bit: 8
    in_bit: 8
    out_bit: 8
debug:
  verbose: 1
  verboise: 1
dst_scheduler: None[0m
[38;21m2024-11-12 20:50:33,604 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:33,706 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:33,965 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:33,991 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:33,991 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,003 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,004 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,004 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,011 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,011 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,012 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,055 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,056 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,057 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,058 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,059 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,059 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,060 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,061 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,061 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,068 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,069 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,070 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,071 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,072 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,072 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,073 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,073 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,074 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,075 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,076 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,076 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,078 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,078 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,079 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,079 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,080 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,081 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,082 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,083 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,083 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,100 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,101 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,101 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,103 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,113 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,114 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,134 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,134 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,135 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,143 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,160 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,160 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,161 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,245 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,245 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,247 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,249 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,249 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,250 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,304 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,304 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,305 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,306 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,306 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,306 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,307 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,308 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,309 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,309 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,310 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,310 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,311 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,311 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,312 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,312 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,313 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,313 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,314 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,315 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,315 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,316 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,316 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,317 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,317 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,318 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,318 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,319 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,320 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,320 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,321 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,322 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,322 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,322 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,323 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,323 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,324 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,325 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,325 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,326 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,326 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,327 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,328 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,328 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,329 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,329 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,330 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,330 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,332 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,332 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,333 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,334 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,334 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,335 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,335 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,336 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,336 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,337 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,337 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,338 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,339 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,339 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,339 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,340 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,341 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,341 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,342 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,342 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,343 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,343 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,344 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,344 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,345 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,346 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,346 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,347 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,347 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,348 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,348 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,349 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,349 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,350 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,350 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,351 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,351 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,352 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,352 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,353 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,354 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,354 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,355 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,355 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,356 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,357 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,357 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,358 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,359 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,359 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,359 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,360 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,361 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,363 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,370 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,370 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,373 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,373 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,374 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,375 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,376 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,376 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,377 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,377 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,378 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,380 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,380 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,381 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,382 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,383 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,384 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,385 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,386 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,387 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,387 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,388 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,388 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,389 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,390 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,391 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,391 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,392 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,392 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,393 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,393 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,394 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,397 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,400 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,401 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,401 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,402 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,402 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,403 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,404 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,405 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,406 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,406 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,407 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,407 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,408 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,409 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,409 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,410 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,410 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,411 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,411 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,413 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,413 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,415 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,415 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,416 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,416 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,417 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,417 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,419 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,419 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,420 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,420 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,421 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,421 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,422 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,423 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,423 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,425 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,425 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,426 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,426 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,427 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,427 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,428 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,429 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,429 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,430 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,430 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,432 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,433 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,433 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,435 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,435 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,436 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,437 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,438 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,438 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,439 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,440 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,441 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,442 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,443 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,443 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,445 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,446 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,447 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,448 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,448 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,449 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,449 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,450 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,451 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,458 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,465 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,466 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,467 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,469 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,469 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,470 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,471 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,472 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,472 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,473 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,474 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,474 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,475 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,475 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,476 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,477 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,478 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,478 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,479 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,479 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,480 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,481 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,481 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,482 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,483 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,483 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,484 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,484 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,485 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,485 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,487 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,487 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,487 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,488 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,489 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,489 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,490 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,490 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,491 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,492 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,492 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,494 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,495 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,495 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,496 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,497 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,498 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,499 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,500 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,501 - utils.py[line:951] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 20:50:34,501 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 20:50:34,504 - train.py[line:270] - INFO: QMobileViT(
  (conv1): ConvBlock(
    (conv): QConv2d(
      3, 16, kernel_size=(3, 3), stride=(2, 2), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
    (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU6(inplace=True)
  )
  (mobvit_blocks): Sequential(
    (0): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            16, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            64, 32, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            32, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            128, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            256, 96, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (4): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          96, 144, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-1): 2 x QTransformerMoEEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                144, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 144, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (gate): Linear(in_features=144, out_features=8, bias=False)
          (experts): ModuleList(
            (0-7): 8 x MlpExpert(
              (activation): ReLU()
              (linear1): LinearBlock(
                (linear): QLinear(
                  144, 288, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                  (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                  (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                  (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                )
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): LinearBlock(
                (linear): QLinear(
                  288, 144, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                  (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                  (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                  (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                )
              )
            )
          )
          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          144, 96, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            96, 384, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            384, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (6): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          128, 192, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-3): 4 x QTransformerEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                192, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 192, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (linear1): LinearBlock(
            (linear): QLinear(
              192, 768, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): LinearBlock(
            (linear): QLinear(
              768, 192, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (activation): ReLU()
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          192, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            512, 160, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (8): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          160, 240, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-2): 3 x QTransformerEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                240, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 240, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (linear1): LinearBlock(
            (linear): QLinear(
              240, 960, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): LinearBlock(
            (linear): QLinear(
              960, 240, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
          (activation): ReLU()
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          240, 160, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (conv_last): ConvBlock(
    (conv): QConv2d(
      160, 640, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
    (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU6(inplace=True)
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (final_layer): LinearBlock(
    (linear): QLinear(
      640, 10, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
  )
)[0m
Files already downloaded and verified
Files already downloaded and verified
set weight_decay to 0 for ['conv1.conv.input_quantizer', 'conv1.conv.input_quantizer', 'conv1.conv.weight_quantizer', 'conv1.conv.output_quantizer', 'conv1.conv.output_quantizer', 'conv1.norm', 'mobvit_blocks.0.conv.0.conv.input_quantizer', 'mobvit_blocks.0.conv.0.conv.input_quantizer', 'mobvit_blocks.0.conv.0.conv.weight_quantizer', 'mobvit_blocks.0.conv.0.conv.output_quantizer', 'mobvit_blocks.0.conv.0.conv.output_quantizer', 'mobvit_blocks.0.conv.0.norm', 'mobvit_blocks.0.conv.1.conv.input_quantizer', 'mobvit_blocks.0.conv.1.conv.input_quantizer', 'mobvit_blocks.0.conv.1.conv.weight_quantizer', 'mobvit_blocks.0.conv.1.conv.output_quantizer', 'mobvit_blocks.0.conv.1.conv.output_quantizer', 'mobvit_blocks.0.conv.1.norm', 'mobvit_blocks.0.conv.2.conv.input_quantizer', 'mobvit_blocks.0.conv.2.conv.input_quantizer', 'mobvit_blocks.0.conv.2.conv.weight_quantizer', 'mobvit_blocks.0.conv.2.conv.output_quantizer', 'mobvit_blocks.0.conv.2.conv.output_quantizer', 'mobvit_blocks.0.conv.2.norm', 'mobvit_blocks.1.conv.0.conv.input_quantizer', 'mobvit_blocks.1.conv.0.conv.input_quantizer', 'mobvit_blocks.1.conv.0.conv.weight_quantizer', 'mobvit_blocks.1.conv.0.conv.output_quantizer', 'mobvit_blocks.1.conv.0.conv.output_quantizer', 'mobvit_blocks.1.conv.0.norm', 'mobvit_blocks.1.conv.1.conv.input_quantizer', 'mobvit_blocks.1.conv.1.conv.input_quantizer', 'mobvit_blocks.1.conv.1.conv.weight_quantizer', 'mobvit_blocks.1.conv.1.conv.output_quantizer', 'mobvit_blocks.1.conv.1.conv.output_quantizer', 'mobvit_blocks.1.conv.1.norm', 'mobvit_blocks.1.conv.2.conv.input_quantizer', 'mobvit_blocks.1.conv.2.conv.input_quantizer', 'mobvit_blocks.1.conv.2.conv.weight_quantizer', 'mobvit_blocks.1.conv.2.conv.output_quantizer', 'mobvit_blocks.1.conv.2.conv.output_quantizer', 'mobvit_blocks.1.conv.2.norm', 'mobvit_blocks.2.conv.0.conv.input_quantizer', 'mobvit_blocks.2.conv.0.conv.input_quantizer', 'mobvit_blocks.2.conv.0.conv.weight_quantizer', 'mobvit_blocks.2.conv.0.conv.output_quantizer', 'mobvit_blocks.2.conv.0.conv.output_quantizer', 'mobvit_blocks.2.conv.0.norm', 'mobvit_blocks.2.conv.1.conv.input_quantizer', 'mobvit_blocks.2.conv.1.conv.input_quantizer', 'mobvit_blocks.2.conv.1.conv.weight_quantizer', 'mobvit_blocks.2.conv.1.conv.output_quantizer', 'mobvit_blocks.2.conv.1.conv.output_quantizer', 'mobvit_blocks.2.conv.1.norm', 'mobvit_blocks.2.conv.2.conv.input_quantizer', 'mobvit_blocks.2.conv.2.conv.input_quantizer', 'mobvit_blocks.2.conv.2.conv.weight_quantizer', 'mobvit_blocks.2.conv.2.conv.output_quantizer', 'mobvit_blocks.2.conv.2.conv.output_quantizer', 'mobvit_blocks.2.conv.2.norm', 'mobvit_blocks.3.conv.0.conv.input_quantizer', 'mobvit_blocks.3.conv.0.conv.input_quantizer', 'mobvit_blocks.3.conv.0.conv.weight_quantizer', 'mobvit_blocks.3.conv.0.conv.output_quantizer', 'mobvit_blocks.3.conv.0.conv.output_quantizer', 'mobvit_blocks.3.conv.0.norm', 'mobvit_blocks.3.conv.1.conv.input_quantizer', 'mobvit_blocks.3.conv.1.conv.input_quantizer', 'mobvit_blocks.3.conv.1.conv.weight_quantizer', 'mobvit_blocks.3.conv.1.conv.output_quantizer', 'mobvit_blocks.3.conv.1.conv.output_quantizer', 'mobvit_blocks.3.conv.1.norm', 'mobvit_blocks.3.conv.2.conv.input_quantizer', 'mobvit_blocks.3.conv.2.conv.input_quantizer', 'mobvit_blocks.3.conv.2.conv.weight_quantizer', 'mobvit_blocks.3.conv.2.conv.output_quantizer', 'mobvit_blocks.3.conv.2.conv.output_quantizer', 'mobvit_blocks.3.conv.2.norm', 'mobvit_blocks.4.conv1.conv.input_quantizer', 'mobvit_blocks.4.conv1.conv.input_quantizer', 'mobvit_blocks.4.conv1.conv.weight_quantizer', 'mobvit_blocks.4.conv1.conv.output_quantizer', 'mobvit_blocks.4.conv1.conv.output_quantizer', 'mobvit_blocks.4.conv1.norm', 'mobvit_blocks.4.conv2.conv.input_quantizer', 'mobvit_blocks.4.conv2.conv.input_quantizer', 'mobvit_blocks.4.conv2.conv.weight_quantizer', 'mobvit_blocks.4.conv2.conv.output_quantizer', 'mobvit_blocks.4.conv2.conv.output_quantizer', 'mobvit_blocks.4.conv2.norm', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.conv3.conv.input_quantizer', 'mobvit_blocks.4.conv3.conv.input_quantizer', 'mobvit_blocks.4.conv3.conv.weight_quantizer', 'mobvit_blocks.4.conv3.conv.output_quantizer', 'mobvit_blocks.4.conv3.conv.output_quantizer', 'mobvit_blocks.4.conv3.norm', 'mobvit_blocks.4.conv4.conv.input_quantizer', 'mobvit_blocks.4.conv4.conv.input_quantizer', 'mobvit_blocks.4.conv4.conv.weight_quantizer', 'mobvit_blocks.4.conv4.conv.output_quantizer', 'mobvit_blocks.4.conv4.conv.output_quantizer', 'mobvit_blocks.4.conv4.norm', 'mobvit_blocks.5.conv.0.conv.input_quantizer', 'mobvit_blocks.5.conv.0.conv.input_quantizer', 'mobvit_blocks.5.conv.0.conv.weight_quantizer', 'mobvit_blocks.5.conv.0.conv.output_quantizer', 'mobvit_blocks.5.conv.0.conv.output_quantizer', 'mobvit_blocks.5.conv.0.norm', 'mobvit_blocks.5.conv.1.conv.input_quantizer', 'mobvit_blocks.5.conv.1.conv.input_quantizer', 'mobvit_blocks.5.conv.1.conv.weight_quantizer', 'mobvit_blocks.5.conv.1.conv.output_quantizer', 'mobvit_blocks.5.conv.1.conv.output_quantizer', 'mobvit_blocks.5.conv.1.norm', 'mobvit_blocks.5.conv.2.conv.input_quantizer', 'mobvit_blocks.5.conv.2.conv.input_quantizer', 'mobvit_blocks.5.conv.2.conv.weight_quantizer', 'mobvit_blocks.5.conv.2.conv.output_quantizer', 'mobvit_blocks.5.conv.2.conv.output_quantizer', 'mobvit_blocks.5.conv.2.norm', 'mobvit_blocks.6.conv1.conv.input_quantizer', 'mobvit_blocks.6.conv1.conv.input_quantizer', 'mobvit_blocks.6.conv1.conv.weight_quantizer', 'mobvit_blocks.6.conv1.conv.output_quantizer', 'mobvit_blocks.6.conv1.conv.output_quantizer', 'mobvit_blocks.6.conv1.norm', 'mobvit_blocks.6.conv2.conv.input_quantizer', 'mobvit_blocks.6.conv2.conv.input_quantizer', 'mobvit_blocks.6.conv2.conv.weight_quantizer', 'mobvit_blocks.6.conv2.conv.output_quantizer', 'mobvit_blocks.6.conv2.conv.output_quantizer', 'mobvit_blocks.6.conv2.norm', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.output_quantizer', 'mobvit_blocks.6.conv3.conv.input_quantizer', 'mobvit_blocks.6.conv3.conv.input_quantizer', 'mobvit_blocks.6.conv3.conv.weight_quantizer', 'mobvit_blocks.6.conv3.conv.output_quantizer', 'mobvit_blocks.6.conv3.conv.output_quantizer', 'mobvit_blocks.6.conv3.norm', 'mobvit_blocks.6.conv4.conv.input_quantizer', 'mobvit_blocks.6.conv4.conv.input_quantizer', 'mobvit_blocks.6.conv4.conv.weight_quantizer', 'mobvit_blocks.6.conv4.conv.output_quantizer', 'mobvit_blocks.6.conv4.conv.output_quantizer', 'mobvit_blocks.6.conv4.norm', 'mobvit_blocks.7.conv.0.conv.input_quantizer', 'mobvit_blocks.7.conv.0.conv.input_quantizer', 'mobvit_blocks.7.conv.0.conv.weight_quantizer', 'mobvit_blocks.7.conv.0.conv.output_quantizer', 'mobvit_blocks.7.conv.0.conv.output_quantizer', 'mobvit_blocks.7.conv.0.norm', 'mobvit_blocks.7.conv.1.conv.input_quantizer', 'mobvit_blocks.7.conv.1.conv.input_quantizer', 'mobvit_blocks.7.conv.1.conv.weight_quantizer', 'mobvit_blocks.7.conv.1.conv.output_quantizer', 'mobvit_blocks.7.conv.1.conv.output_quantizer', 'mobvit_blocks.7.conv.1.norm', 'mobvit_blocks.7.conv.2.conv.input_quantizer', 'mobvit_blocks.7.conv.2.conv.input_quantizer', 'mobvit_blocks.7.conv.2.conv.weight_quantizer', 'mobvit_blocks.7.conv.2.conv.output_quantizer', 'mobvit_blocks.7.conv.2.conv.output_quantizer', 'mobvit_blocks.7.conv.2.norm', 'mobvit_blocks.8.conv1.conv.input_quantizer', 'mobvit_blocks.8.conv1.conv.input_quantizer', 'mobvit_blocks.8.conv1.conv.weight_quantizer', 'mobvit_blocks.8.conv1.conv.output_quantizer', 'mobvit_blocks.8.conv1.conv.output_quantizer', 'mobvit_blocks.8.conv1.norm', 'mobvit_blocks.8.conv2.conv.input_quantizer', 'mobvit_blocks.8.conv2.conv.input_quantizer', 'mobvit_blocks.8.conv2.conv.weight_quantizer', 'mobvit_blocks.8.conv2.conv.output_quantizer', 'mobvit_blocks.8.conv2.conv.output_quantizer', 'mobvit_blocks.8.conv2.norm', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.8.conv3.conv.input_quantizer', 'mobvit_blocks.8.conv3.conv.input_quantizer', 'mobvit_blocks.8.conv3.conv.weight_quantizer', 'mobvit_blocks.8.conv3.conv.output_quantizer', 'mobvit_blocks.8.conv3.conv.output_quantizer', 'mobvit_blocks.8.conv3.norm', 'mobvit_blocks.8.conv4.conv.input_quantizer', 'mobvit_blocks.8.conv4.conv.input_quantizer', 'mobvit_blocks.8.conv4.conv.weight_quantizer', 'mobvit_blocks.8.conv4.conv.output_quantizer', 'mobvit_blocks.8.conv4.conv.output_quantizer', 'mobvit_blocks.8.conv4.norm', 'conv_last.conv.input_quantizer', 'conv_last.conv.input_quantizer', 'conv_last.conv.weight_quantizer', 'conv_last.conv.output_quantizer', 'conv_last.conv.output_quantizer', 'conv_last.norm', 'final_layer.linear.input_quantizer', 'final_layer.linear.input_quantizer', 'final_layer.linear.weight_quantizer', 'final_layer.linear.output_quantizer', 'final_layer.linear.output_quantizer']/home/xinyuzh/workspace/DAC2025_Train_public/train.py:312: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  grad_scaler = amp.GradScaler(enabled=getattr(configs.run, "fp16", False))
[38;21m2024-11-12 20:50:34,513 - train.py[line:313] - INFO: Number of parameters: 6120866[0m
[38;21m2024-11-12 20:50:34,513 - train.py[line:318] - INFO: Current checkpoint: ./checkpoint/cifar10/qmobileViTMoE/train/QMobileViT_lr-0.0020_wb-8_run-1.pt[0m
[38;21m2024-11-12 20:50:34,513 - train.py[line:354] - INFO: Experiment starts.[0m

{}
/home/xinyuzh/workspace/DAC2025_Train_public/train.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=grad_scaler._enabled):
[38;21m2024-11-12 20:50:35,277 - train.py[line:120] - INFO: Train Epoch: 1 [      8/  45000 (  0%)] Loss: 2.8287e+00 class Loss: 2.8287e+00[0m
Traceback (most recent call last):
  File "/home/xinyuzh/workspace/DAC2025_Train_public/train.py", line 452, in <module>
    main()
  File "/home/xinyuzh/workspace/DAC2025_Train_public/train.py", line 402, in main
    train(
  File "/home/xinyuzh/workspace/DAC2025_Train_public/train.py", line 62, in train
    output = model(data)
             ^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/quantized_mobileViT.py", line 641, in forward
    x = self.mobvit_blocks(x)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/quantized_mobileViT.py", line 439, in forward
    y = layer(y)
        ^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/quantized_mobileViT.py", line 331, in forward
    expert_out = expert_layer(expert_in) * top_k_scores[top_x, idx].unsqueeze(1)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/quantized_mobileViT.py", line 269, in forward
    src = self.linear2(self.dropout(self.activation(self.linear1(src))))
                                                    ^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/quantized_base.py", line 128, in forward
    x = self.linear(x)
        ^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/layers/quantized_base_layer.py", line 116, in forward
    x = self.input_transform(x)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/layers/quantized_base_layer.py", line 76, in input_transform
    x = self.input_quantizer(x)  # [-alpha, alpha] or [0, alpha]
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xinyuzh/workspace/DAC2025_Train_public/core/models/layers/utils.py", line 859, in forward
    g = 1.0 / (x.data.numel() * self.Qp) ** 0.5
        ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ZeroDivisionError: float division by zero
