/home/xinyuzh/anaconda3/envs/timm/lib/python3.11/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
[38;21m2024-11-12 17:27:03,857 - train.py[line:234] - INFO: dataset:
  name: cifar10
  root: /home/xinyuzh/workspace/dataset/DAC2025_Train_public/cifar10
  train_valid_split_ratio: [0.9, 0.1]
  train_valid_split_seed: 1
  resize_mode: bicubic
  center_crop: 256
  n_test_samples: None
  n_valid_samples: None
  num_workers: 2
  img_height: 256
  img_width: 256
  in_channels: 3
  num_classes: 10
  transform: augmented
  shuffle: 0
  augment: None
criterion:
  name: ce
aux_criterion: None
optimizer:
  name: adamw
  lr: 0.002
  weight_decay: 0.0001
  grad_clip_value: 0
scheduler:
  name: cosine
  lr_gamma: 0.99
  lr_min: 2e-05
run:
  experiment: cifar10_qmobilevitmoe_train
  n_epochs: 100
  batch_size: 8
  use_cuda: 1
  gpu_id: 0
  deterministic: 22
  random_state: 42
  log_interval: 200
  train_noise: 0
  grad_clip: False
  max_grad_value: 1
  do_distill: False
  compile: False
quantize:
  weight_bit: 8
  input_bit: 6
noise:
  phase_bias: 0
  phase_noise_std: 0
  gamma_noise_std: 0
  crosstalk_factor: 0
  random_state: 42
  weight_noise_std: 0.0
  output_noise_std: 0
  crosstalk_flag: False
  noise_flag: False
  light_redist: False
  input_power_gating: False
  input_modulation_ER: 6
  output_power_gating: False
  crosstalk_scheduler:
    interv_h: 20
    interv_v: 120
    interv_s: 9
    ps_width: 6
checkpoint:
  save_best_model_k: 3
  checkpoint_dir: cifar10/qmobileViTMoE/train
  model_comment: lr-0.0020_wb-8_run-1
  resume: 0
  restore_checkpoint: 
  no_linear: 0
model:
  name: QMobileViT
  conv_cfg:
    type: QConv2d
    w_bit: 8
    in_bit: 8
    out_bit: 8
  linear_cfg:
    type: QLinear
    w_bit: 8
    in_bit: 8
    out_bit: 8
  norm_cfg:
    type: BN2d
    affine: True
  act_cfg:
    type: ReLU6
    inplace: True
  dim: [144, 192, 240]
  depth: [2, 4, 3]
  channels: [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]
  is_moe: True
  expert_num: 8
  top_k: 2
  expansion: 4
  matmul_cfg:
    type: QMatMul
    w_bit: 8
    in_bit: 8
    out_bit: 8
debug:
  verbose: 1
  verboise: 1
dst_scheduler: None[0m
[38;21m2024-11-12 17:27:06,509 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,573 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,673 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,688 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,691 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,715 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,716 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,718 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,720 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,721 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,722 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,727 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,728 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,729 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,730 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,732 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,733 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,733 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,734 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,734 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,736 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,737 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,738 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,740 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,741 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,742 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,742 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,743 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,744 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,745 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,746 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,747 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,747 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,748 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,749 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,750 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,751 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,752 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,753 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,754 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,755 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,765 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,766 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,766 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,768 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,776 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,777 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,782 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,783 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,783 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,784 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,799 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,799 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,800 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,865 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,866 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,866 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,867 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,868 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,868 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,920 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,921 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,922 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,922 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,923 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,924 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,924 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,925 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,926 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,927 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,927 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,928 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,928 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,929 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,930 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,930 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,931 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,931 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,932 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,933 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,934 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,934 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,935 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,935 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,936 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,937 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,937 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,938 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,939 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,939 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,940 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,941 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,941 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,942 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,943 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,943 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,944 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,945 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,945 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,946 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,946 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,947 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,948 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,949 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,949 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,950 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,951 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,951 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,954 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,955 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,956 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,957 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,958 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,958 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,959 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,960 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,960 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,961 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,961 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,962 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,963 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,964 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,964 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,965 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,965 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,966 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,967 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,967 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,968 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,969 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,969 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,970 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,971 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,971 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,972 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,973 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,973 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,974 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,975 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,975 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,976 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,977 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,977 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,978 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,978 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,979 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,980 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,980 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,981 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,982 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,982 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,983 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,983 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,984 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,985 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,985 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,986 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,987 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,987 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,988 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,989 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,992 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,996 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:06,997 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:06,999 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,000 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,000 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,002 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,002 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,003 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,003 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,004 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,005 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,006 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,007 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,008 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,009 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,010 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,011 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,012 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,013 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,013 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,014 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,015 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,015 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,016 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,018 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,019 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,019 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,020 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,020 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,021 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,022 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,023 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,023 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,024 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,025 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,025 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,026 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,026 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,027 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,029 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,030 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,030 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,031 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,031 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,032 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,033 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,034 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,034 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,035 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,036 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,036 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,037 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,037 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,038 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,040 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,041 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,041 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,042 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,043 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,043 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,044 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,045 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,046 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,046 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,047 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,047 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,048 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,049 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,049 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,052 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,052 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,053 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,054 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,054 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,056 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,057 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,057 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,058 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,059 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,059 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,061 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,062 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,062 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,064 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,065 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,065 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,066 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,067 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,068 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,069 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,069 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,070 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,071 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,072 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,072 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,074 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,075 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,075 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,077 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,077 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,078 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,078 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,079 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,080 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,080 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,081 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,082 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,082 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,083 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,084 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,084 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,085 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,086 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,086 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,087 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,088 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,088 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,089 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,089 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,090 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,091 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,092 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,092 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,093 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,094 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,094 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,095 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,096 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,096 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,097 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,098 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,098 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,099 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,099 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,100 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,101 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,102 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,102 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,103 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,103 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,105 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,105 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,106 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,107 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,107 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,108 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,109 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,110 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,111 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,112 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,113 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,114 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,115 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,116 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,116 - utils.py[line:952] - INFO: LSQ Weight quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: False[0m
[38;21m2024-11-12 17:27:07,117 - utils.py[line:831] - INFO: LSQ Act quantizer: (mode: tensor_wise): initialize weight scale for int8 quantization with offset: True[0m
[38;21m2024-11-12 17:27:07,120 - train.py[line:270] - INFO: QMobileViT(
  (conv1): ConvBlock(
    (conv): QConv2d(
      3, 16, kernel_size=(3, 3), stride=(2, 2), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
    (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU6(inplace=True)
  )
  (mobvit_blocks): Sequential(
    (0): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            16, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            64, 32, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            32, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            128, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            256, 96, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (4): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          96, 144, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-1): 2 x QTransformerMoEEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                144, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 144, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (gate): Linear(in_features=144, out_features=8, bias=False)
          (experts): ModuleList(
            (0-7): 8 x MlpExpert(
              (activation): ReLU()
              (linear1): LinearBlock(
                (linear): QLinear(
                  144, 288, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                  (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                  (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                  (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                )
              )
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): LinearBlock(
                (linear): QLinear(
                  288, 144, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                  (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                  (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                  (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                )
              )
            )
          )
          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          144, 96, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            96, 384, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            384, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (6): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          128, 192, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-3): 4 x QTransformerEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                192, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 192, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (linear1): LinearBlock(
            (linear): QLinear(
              192, 768, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): LinearBlock(
            (linear): QLinear(
              768, 192, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (activation): ReLU()
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          192, 128, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): ConvBlock(
          (conv): QConv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (1): ConvBlock(
          (conv): QConv2d(
            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU6(inplace=True)
        )
        (2): ConvBlock(
          (conv): QConv2d(
            512, 160, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
            (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
            (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          )
          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (8): QMobileViTBlock(
      (conv1): ConvBlock(
        (conv): QConv2d(
          160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv2): ConvBlock(
        (conv): QConv2d(
          160, 240, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (transformer_layers): ModuleList(
        (0-2): 3 x QTransformerEncoderLayer(
          (self_attn): QAttention(
            Quantized_Attention
            (qkv): LinearBlock(
              (linear): QLinear(
                240, 96, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (proj): LinearBlock(
              (linear): QLinear(
                32, 240, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
            (quantized_matmul): MatMulBlock(
              (matmul): QMatMul(
                QuantizedMatMul, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
                (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
                (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
                (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              )
            )
          )
          (linear1): LinearBlock(
            (linear): QLinear(
              240, 960, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): LinearBlock(
            (linear): QLinear(
              960, 240, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
              (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
              (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
              (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
            )
          )
          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)
          (activation): ReLU()
        )
      )
      (conv3): ConvBlock(
        (conv): QConv2d(
          240, 160, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
      (conv4): ConvBlock(
        (conv): QConv2d(
          320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
          (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
          (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
          (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
        )
        (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activation): ReLU6(inplace=True)
      )
    )
  )
  (conv_last): ConvBlock(
    (conv): QConv2d(
      160, 640, kernel_size=(1, 1), stride=(1, 1), in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
    (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (activation): ReLU6(inplace=True)
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (final_layer): LinearBlock(
    (linear): QLinear(
      640, 10, in_bits=8, w_bits=8, out_bits=8, input_noise_std=0, weight_noise_std=0, output_noise_std=0
      (input_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
      (weight_quantizer): WeightQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': True, 'offset': False})
      (output_quantizer): ActQuantizer_LSQ({'nbits': 8, 'mode': 'tensor_wise', 'signed': False, 'offset': True})
    )
  )
)[0m
Files already downloaded and verified
Files already downloaded and verified
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
cuda:0
set weight_decay to 0 for ['conv1.conv.input_quantizer', 'conv1.conv.input_quantizer', 'conv1.conv.weight_quantizer', 'conv1.conv.output_quantizer', 'conv1.conv.output_quantizer', 'conv1.norm', 'mobvit_blocks.0.conv.0.conv.input_quantizer', 'mobvit_blocks.0.conv.0.conv.input_quantizer', 'mobvit_blocks.0.conv.0.conv.weight_quantizer', 'mobvit_blocks.0.conv.0.conv.output_quantizer', 'mobvit_blocks.0.conv.0.conv.output_quantizer', 'mobvit_blocks.0.conv.0.norm', 'mobvit_blocks.0.conv.1.conv.input_quantizer', 'mobvit_blocks.0.conv.1.conv.input_quantizer', 'mobvit_blocks.0.conv.1.conv.weight_quantizer', 'mobvit_blocks.0.conv.1.conv.output_quantizer', 'mobvit_blocks.0.conv.1.conv.output_quantizer', 'mobvit_blocks.0.conv.1.norm', 'mobvit_blocks.0.conv.2.conv.input_quantizer', 'mobvit_blocks.0.conv.2.conv.input_quantizer', 'mobvit_blocks.0.conv.2.conv.weight_quantizer', 'mobvit_blocks.0.conv.2.conv.output_quantizer', 'mobvit_blocks.0.conv.2.conv.output_quantizer', 'mobvit_blocks.0.conv.2.norm', 'mobvit_blocks.1.conv.0.conv.input_quantizer', 'mobvit_blocks.1.conv.0.conv.input_quantizer', 'mobvit_blocks.1.conv.0.conv.weight_quantizer', 'mobvit_blocks.1.conv.0.conv.output_quantizer', 'mobvit_blocks.1.conv.0.conv.output_quantizer', 'mobvit_blocks.1.conv.0.norm', 'mobvit_blocks.1.conv.1.conv.input_quantizer', 'mobvit_blocks.1.conv.1.conv.input_quantizer', 'mobvit_blocks.1.conv.1.conv.weight_quantizer', 'mobvit_blocks.1.conv.1.conv.output_quantizer', 'mobvit_blocks.1.conv.1.conv.output_quantizer', 'mobvit_blocks.1.conv.1.norm', 'mobvit_blocks.1.conv.2.conv.input_quantizer', 'mobvit_blocks.1.conv.2.conv.input_quantizer', 'mobvit_blocks.1.conv.2.conv.weight_quantizer', 'mobvit_blocks.1.conv.2.conv.output_quantizer', 'mobvit_blocks.1.conv.2.conv.output_quantizer', 'mobvit_blocks.1.conv.2.norm', 'mobvit_blocks.2.conv.0.conv.input_quantizer', 'mobvit_blocks.2.conv.0.conv.input_quantizer', 'mobvit_blocks.2.conv.0.conv.weight_quantizer', 'mobvit_blocks.2.conv.0.conv.output_quantizer', 'mobvit_blocks.2.conv.0.conv.output_quantizer', 'mobvit_blocks.2.conv.0.norm', 'mobvit_blocks.2.conv.1.conv.input_quantizer', 'mobvit_blocks.2.conv.1.conv.input_quantizer', 'mobvit_blocks.2.conv.1.conv.weight_quantizer', 'mobvit_blocks.2.conv.1.conv.output_quantizer', 'mobvit_blocks.2.conv.1.conv.output_quantizer', 'mobvit_blocks.2.conv.1.norm', 'mobvit_blocks.2.conv.2.conv.input_quantizer', 'mobvit_blocks.2.conv.2.conv.input_quantizer', 'mobvit_blocks.2.conv.2.conv.weight_quantizer', 'mobvit_blocks.2.conv.2.conv.output_quantizer', 'mobvit_blocks.2.conv.2.conv.output_quantizer', 'mobvit_blocks.2.conv.2.norm', 'mobvit_blocks.3.conv.0.conv.input_quantizer', 'mobvit_blocks.3.conv.0.conv.input_quantizer', 'mobvit_blocks.3.conv.0.conv.weight_quantizer', 'mobvit_blocks.3.conv.0.conv.output_quantizer', 'mobvit_blocks.3.conv.0.conv.output_quantizer', 'mobvit_blocks.3.conv.0.norm', 'mobvit_blocks.3.conv.1.conv.input_quantizer', 'mobvit_blocks.3.conv.1.conv.input_quantizer', 'mobvit_blocks.3.conv.1.conv.weight_quantizer', 'mobvit_blocks.3.conv.1.conv.output_quantizer', 'mobvit_blocks.3.conv.1.conv.output_quantizer', 'mobvit_blocks.3.conv.1.norm', 'mobvit_blocks.3.conv.2.conv.input_quantizer', 'mobvit_blocks.3.conv.2.conv.input_quantizer', 'mobvit_blocks.3.conv.2.conv.weight_quantizer', 'mobvit_blocks.3.conv.2.conv.output_quantizer', 'mobvit_blocks.3.conv.2.conv.output_quantizer', 'mobvit_blocks.3.conv.2.norm', 'mobvit_blocks.4.conv1.conv.input_quantizer', 'mobvit_blocks.4.conv1.conv.input_quantizer', 'mobvit_blocks.4.conv1.conv.weight_quantizer', 'mobvit_blocks.4.conv1.conv.output_quantizer', 'mobvit_blocks.4.conv1.conv.output_quantizer', 'mobvit_blocks.4.conv1.norm', 'mobvit_blocks.4.conv2.conv.input_quantizer', 'mobvit_blocks.4.conv2.conv.input_quantizer', 'mobvit_blocks.4.conv2.conv.weight_quantizer', 'mobvit_blocks.4.conv2.conv.output_quantizer', 'mobvit_blocks.4.conv2.conv.output_quantizer', 'mobvit_blocks.4.conv2.norm', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.0.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.0.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.1.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.2.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.3.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.4.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.5.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.6.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear1.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.input_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.weight_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.transformer_layers.1.experts.7.linear2.linear.output_quantizer', 'mobvit_blocks.4.conv3.conv.input_quantizer', 'mobvit_blocks.4.conv3.conv.input_quantizer', 'mobvit_blocks.4.conv3.conv.weight_quantizer', 'mobvit_blocks.4.conv3.conv.output_quantizer', 'mobvit_blocks.4.conv3.conv.output_quantizer', 'mobvit_blocks.4.conv3.norm', 'mobvit_blocks.4.conv4.conv.input_quantizer', 'mobvit_blocks.4.conv4.conv.input_quantizer', 'mobvit_blocks.4.conv4.conv.weight_quantizer', 'mobvit_blocks.4.conv4.conv.output_quantizer', 'mobvit_blocks.4.conv4.conv.output_quantizer', 'mobvit_blocks.4.conv4.norm', 'mobvit_blocks.5.conv.0.conv.input_quantizer', 'mobvit_blocks.5.conv.0.conv.input_quantizer', 'mobvit_blocks.5.conv.0.conv.weight_quantizer', 'mobvit_blocks.5.conv.0.conv.output_quantizer', 'mobvit_blocks.5.conv.0.conv.output_quantizer', 'mobvit_blocks.5.conv.0.norm', 'mobvit_blocks.5.conv.1.conv.input_quantizer', 'mobvit_blocks.5.conv.1.conv.input_quantizer', 'mobvit_blocks.5.conv.1.conv.weight_quantizer', 'mobvit_blocks.5.conv.1.conv.output_quantizer', 'mobvit_blocks.5.conv.1.conv.output_quantizer', 'mobvit_blocks.5.conv.1.norm', 'mobvit_blocks.5.conv.2.conv.input_quantizer', 'mobvit_blocks.5.conv.2.conv.input_quantizer', 'mobvit_blocks.5.conv.2.conv.weight_quantizer', 'mobvit_blocks.5.conv.2.conv.output_quantizer', 'mobvit_blocks.5.conv.2.conv.output_quantizer', 'mobvit_blocks.5.conv.2.norm', 'mobvit_blocks.6.conv1.conv.input_quantizer', 'mobvit_blocks.6.conv1.conv.input_quantizer', 'mobvit_blocks.6.conv1.conv.weight_quantizer', 'mobvit_blocks.6.conv1.conv.output_quantizer', 'mobvit_blocks.6.conv1.conv.output_quantizer', 'mobvit_blocks.6.conv1.norm', 'mobvit_blocks.6.conv2.conv.input_quantizer', 'mobvit_blocks.6.conv2.conv.input_quantizer', 'mobvit_blocks.6.conv2.conv.weight_quantizer', 'mobvit_blocks.6.conv2.conv.output_quantizer', 'mobvit_blocks.6.conv2.conv.output_quantizer', 'mobvit_blocks.6.conv2.norm', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear1.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.input_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.weight_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.output_quantizer', 'mobvit_blocks.6.transformer_layers.3.linear2.linear.output_quantizer', 'mobvit_blocks.6.conv3.conv.input_quantizer', 'mobvit_blocks.6.conv3.conv.input_quantizer', 'mobvit_blocks.6.conv3.conv.weight_quantizer', 'mobvit_blocks.6.conv3.conv.output_quantizer', 'mobvit_blocks.6.conv3.conv.output_quantizer', 'mobvit_blocks.6.conv3.norm', 'mobvit_blocks.6.conv4.conv.input_quantizer', 'mobvit_blocks.6.conv4.conv.input_quantizer', 'mobvit_blocks.6.conv4.conv.weight_quantizer', 'mobvit_blocks.6.conv4.conv.output_quantizer', 'mobvit_blocks.6.conv4.conv.output_quantizer', 'mobvit_blocks.6.conv4.norm', 'mobvit_blocks.7.conv.0.conv.input_quantizer', 'mobvit_blocks.7.conv.0.conv.input_quantizer', 'mobvit_blocks.7.conv.0.conv.weight_quantizer', 'mobvit_blocks.7.conv.0.conv.output_quantizer', 'mobvit_blocks.7.conv.0.conv.output_quantizer', 'mobvit_blocks.7.conv.0.norm', 'mobvit_blocks.7.conv.1.conv.input_quantizer', 'mobvit_blocks.7.conv.1.conv.input_quantizer', 'mobvit_blocks.7.conv.1.conv.weight_quantizer', 'mobvit_blocks.7.conv.1.conv.output_quantizer', 'mobvit_blocks.7.conv.1.conv.output_quantizer', 'mobvit_blocks.7.conv.1.norm', 'mobvit_blocks.7.conv.2.conv.input_quantizer', 'mobvit_blocks.7.conv.2.conv.input_quantizer', 'mobvit_blocks.7.conv.2.conv.weight_quantizer', 'mobvit_blocks.7.conv.2.conv.output_quantizer', 'mobvit_blocks.7.conv.2.conv.output_quantizer', 'mobvit_blocks.7.conv.2.norm', 'mobvit_blocks.8.conv1.conv.input_quantizer', 'mobvit_blocks.8.conv1.conv.input_quantizer', 'mobvit_blocks.8.conv1.conv.weight_quantizer', 'mobvit_blocks.8.conv1.conv.output_quantizer', 'mobvit_blocks.8.conv1.conv.output_quantizer', 'mobvit_blocks.8.conv1.norm', 'mobvit_blocks.8.conv2.conv.input_quantizer', 'mobvit_blocks.8.conv2.conv.input_quantizer', 'mobvit_blocks.8.conv2.conv.weight_quantizer', 'mobvit_blocks.8.conv2.conv.output_quantizer', 'mobvit_blocks.8.conv2.conv.output_quantizer', 'mobvit_blocks.8.conv2.norm', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.0.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.1.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.qkv.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.proj.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.self_attn.quantized_matmul.matmul.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear1.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.input_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.weight_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.8.transformer_layers.2.linear2.linear.output_quantizer', 'mobvit_blocks.8.conv3.conv.input_quantizer', 'mobvit_blocks.8.conv3.conv.input_quantizer', 'mobvit_blocks.8.conv3.conv.weight_quantizer', 'mobvit_blocks.8.conv3.conv.output_quantizer', 'mobvit_blocks.8.conv3.conv.output_quantizer', 'mobvit_blocks.8.conv3.norm', 'mobvit_blocks.8.conv4.conv.input_quantizer', 'mobvit_blocks.8.conv4.conv.input_quantizer', 'mobvit_blocks.8.conv4.conv.weight_quantizer', 'mobvit_blocks.8.conv4.conv.output_quantizer', 'mobvit_blocks.8.conv4.conv.output_quantizer', 'mobvit_blocks.8.conv4.norm', 'conv_last.conv.input_quantizer', 'conv_last.conv.input_quantizer', 'conv_last.conv.weight_quantizer', 'conv_last.conv.output_quantizer', 'conv_last.conv.output_quantizer', 'conv_last.norm', 'final_layer.linear.input_quantizer', 'final_layer.linear.input_quantizer', 'final_layer.linear.weight_quantizer', 'final_layer.linear.output_quantizer', 'final_layer.linear.output_quantizer']/home/xinyuzh/workspace/DAC2025_Train_public/train.py:312: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  grad_scaler = amp.GradScaler(enabled=getattr(configs.run, "fp16", False))
[38;21m2024-11-12 17:27:07,130 - train.py[line:313] - INFO: Number of parameters: 6120866[0m
[38;21m2024-11-12 17:27:07,130 - train.py[line:318] - INFO: Current checkpoint: ./checkpoint/cifar10/qmobileViTMoE/train/QMobileViT_lr-0.0020_wb-8_run-1.pt[0m
[38;21m2024-11-12 17:27:07,130 - train.py[line:354] - INFO: Experiment starts.[0m

{}
/home/xinyuzh/workspace/DAC2025_Train_public/train.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp.autocast(enabled=grad_scaler._enabled):
[38;21m2024-11-12 17:27:07,901 - train.py[line:120] - INFO: Train Epoch: 1 [      8/  45000 (  0%)] Loss: 2.6524e+00 class Loss: 2.6524e+00[0m
[33;21m2024-11-12 17:27:41,058 - train.py[line:448] - WARNING: Ctrl-C Stopped[0m
